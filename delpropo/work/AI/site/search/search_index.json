{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Information Documentation","text":""},{"location":"#description","title":"Description","text":"<p>Welcome to the AI Information documentation site! This is a comprehensive resource for AI-related information, tools, and configurations.</p>"},{"location":"#available-documentation","title":"Available Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started Guide - Quick start guide to get you up and running</li> </ul>"},{"location":"#configuration-setup","title":"Configuration &amp; Setup","text":"<ul> <li>Bashrc Configuration - Shell configuration and environment setup</li> <li>File Location YAML Prompt - YAML configuration prompts and file location guides</li> </ul>"},{"location":"#ai-llm-resources","title":"AI &amp; LLM Resources","text":"<ul> <li>LLM Information - Large Language Model information and resources</li> <li>Considerations - Important considerations for AI implementations</li> </ul>"},{"location":"#development-scripting","title":"Development &amp; Scripting","text":"<ul> <li>Mercury Shell Scripting - Comprehensive shell scripting guide for Mercury environment</li> </ul>"},{"location":"#planning-migration","title":"Planning &amp; Migration","text":"<ul> <li>Transition Plan - Planning and transition documentation</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Category Documents Setup &amp; Config Bashrc, File Locations AI Resources LLM Info, Considerations Development Mercury Scripting Planning Transition Plan Getting Started Quick Start"},{"location":"#commands","title":"Commands","text":"<p>The Makefile contains the central entry points for common tasks related to this project.</p>"},{"location":"Considerations/","title":"Considerations","text":"<p>Paying for a github account for the lab so we can have both public and private repositories</p> <p>Examples of workflow software</p> <ul> <li>Galaxy</li> <li>KNIME</li> <li>Nextflow</li> <li>Snakemake</li> <li>GenPipes v6.0.0 Documentation \u2014 GenPipes 6.0.0 documentation</li> <li>Bpipe</li> <li>Pachyderm</li> <li>SciPipe</li> <li>Luigi</li> <li>WDL</li> <li>CWL</li> <li>Toil</li> </ul>"},{"location":"bashrc/","title":"Understanding the .bashrc File","text":"<p>The <code>.bashrc</code> file is a shell script located in your home directory (<code>~/.bashrc</code>) that runs every time you open a new interactive terminal session. Its primary purpose is to customize your command-line environment, making it more powerful and efficient for your specific workflow.</p> <p>You can use it to: - Set environment variables that programs can use. - Create aliases, which are shortcuts for longer commands. - Define custom functions to perform more complex tasks.</p>"},{"location":"bashrc/#dissecting-an-example-bashrc-file","title":"Dissecting an Example .bashrc File","text":"<p>Let's look at your <code>.bashrc</code> file to see how these concepts are applied in practice.</p> <pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n# User specific environment\nif ! [[ \"$PATH\" =~ \"$HOME/.local/bin:$HOME/bin:\" ]]\nthen\n    PATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nfi\nexport PATH\n\n# User specific aliases and functions\nfunction mkdircd () { mkdir -p \"$@\" &amp;&amp; eval cd \"\\\"\\$$#\\\"\"; pwd; }\nalias l=\"less -S\"\nalias scratch=\"cd /scratch/margit_root/margit0/delpropo\"\nalias ns=\"cd /nfs/turbo/umms-mblabns\"\nalias jobst=\"sq | grep -v JOBID | awk '{print $1}' | xargs -I {} my_job_statistics {}\"\n</code></pre>"},{"location":"bashrc/#environment-variables-the-path","title":"Environment Variables: The <code>PATH</code>","text":"<p><pre><code>if ! [[ \"$PATH\" =~ \"$HOME/.local/bin:$HOME/bin:\" ]]\nthen\n    PATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nfi\nexport PATH\n</code></pre> This section modifies the <code>PATH</code> environment variable, which tells your shell where to look for executable programs. By adding <code>$HOME/.local/bin</code> and <code>$HOME/bin</code>, you can run scripts or programs you've placed in those directories without having to type the full path.</p>"},{"location":"bashrc/#aliases-your-command-line-shortcuts","title":"Aliases: Your Command-Line Shortcuts","text":"<p>Aliases are custom shortcuts that replace a longer command. They are defined using the <code>alias name=\"command\"</code> syntax. Here are the aliases from your file explained:</p> <ul> <li> <p><code>alias l=\"less -S\"</code>   A convenient shortcut for viewing files. The <code>-S</code> flag for <code>less</code> prevents long lines from wrapping, which is useful for looking at wide data files.</p> </li> <li> <p><code>alias scratch=\"cd /scratch/margit_root/margit0/delpropo\"</code>   Instantly navigates you to your personal scratch directory. Instead of typing the long path, you just type <code>scratch</code>.</p> </li> <li> <p><code>alias ns=\"cd /nfs/turbo/umms-mblabns\"</code>   Quickly changes the directory to a specific network file system location.</p> </li> <li> <p><code>alias jobst=\"sq | grep -v JOBID | awk '{print $1}' | xargs -I {} my_job_statistics {}\"</code>   This is a more advanced alias that creates a custom command for checking job statistics, likely in a high-performance computing (HPC) environment using a scheduler like Slurm. It gets a list of your job IDs and runs a <code>my_job_statistics</code> command on each one.</p> </li> </ul>"},{"location":"bashrc/#custom-functions-more-powerful-than-aliases","title":"Custom Functions: More Powerful Than Aliases","text":"<p>Functions are like advanced aliases that can accept arguments and perform more complex logic.</p> <ul> <li><code>function mkdircd () { mkdir -p \"$@\" &amp;&amp; eval cd \"\\\"\\$$#\\\"\"; pwd; }</code>   This is a very useful function that combines two common commands: <code>mkdir</code> (make directory) and <code>cd</code> (change directory). When you run <code>mkdircd new_folder</code>, it will:</li> <li>Create the directory <code>new_folder</code>.</li> <li>Immediately change into <code>new_folder</code>.</li> <li>Print the current working directory (<code>pwd</code>).</li> </ul>"},{"location":"bashrc/#how-to-apply-changes","title":"How to Apply Changes","text":"<p>After you edit your <code>.bashrc</code> file, the changes will not take effect until you either: 1.  Open a new terminal session. 2.  Run <code>source ~/.bashrc</code> in your current terminal to apply the changes immediately.</p> <p>By customizing your <code>.bashrc</code>, you can save time and create a command-line environment tailored perfectly to your needs.</p>"},{"location":"file_location_yaml_prompt/","title":"File Locations","text":""},{"location":"file_location_yaml_prompt/#prompt-instructions-for-document-creation","title":"Prompt Instructions for Document Creation","text":"<p>Role: You are a technical documentation specialist who excels at transforming structured data into clear, professional documents with visual elements.</p> <p>Task: Convert the provided YAML file into a comprehensive, well-organized document that serves as a transition guide for lab resources and computing infrastructure.</p> <p>Requirements:</p> <ol> <li>Document Structure:</li> </ol> <p>- Create a professional title page with document metadata</p> <p>- Include a table of contents with page numbers</p> <p>- Organize content into logical sections with clear headings</p> <p>- Use consistent formatting throughout</p> <ol> <li>Content Organization:</li> </ol> <p>- Transform each YAML section into readable prose with appropriate subheadings</p> <p>- Present file and folder information in easy-to-scan tables</p> <p>- Include executive summary highlighting the most critical items</p> <p>- Add transition priorities (High/Medium/Low) based on content analysis</p> <ol> <li>Visual Elements to Include:</li> </ol> <p>- Storage Architecture Diagram: Show relationships between OneDrive, R Drive, ARC clusters, and Turbo storage</p> <p>- File Location Flowchart: Illustrate how data flows between different systems</p> <p>- Access Method Chart: Visual guide showing how to access each storage system</p> <p>- Priority Matrix: Chart showing criticality vs. frequency of use for key files</p> <p>- Timeline/Handover Checklist: Visual checklist for transition tasks</p> <ol> <li>Tables and Charts:</li> </ol> <p>- Summary table of all storage locations with key details</p> <p>- File inventory table with sortable columns</p> <p>- System comparison chart (Great Lakes vs. Armis2)</p> <p>- Quick reference guide for file paths and access methods</p> <ol> <li>Format Specifications:</li> </ol> <p>- Use professional styling (consistent fonts, spacing, colors)</p> <p>- Include headers and footers with document info</p> <p>- Make it print-friendly with proper page breaks</p> <p>- Ensure all charts and diagrams are clearly labeled</p> <p>- Add callout boxes for critical information</p> <ol> <li>Additional Elements:</li> </ol> <p>- Create a glossary of technical terms (HPC, Turbo, Globus, etc.)</p> <p>- Include troubleshooting section for common access issues</p> <p>- Add contact information placeholders for key systems</p> <p>- Provide next steps/action items for successor</p> <p>Tone: Professional, clear, and accessible to someone unfamiliar with the lab's systems.</p> <p>Output Format: Request the document in [specify your preferred format: Word, PDF, Markdown, etc.]</p> <p>Special Instructions:</p> <ul> <li> <p>Highlight any \"Work in progress\" items that need immediate attention</p> </li> <li> <p>Emphasize the relationship between inventory files and lab operations</p> </li> <li> <p>Make the computing cluster information accessible to non-technical users</p> </li> <li> <p>Include visual cues (icons, colors) to distinguish between different types of storage systems</p> </li> </ul> <p>Data to Process: [Insert your YAML content here]</p> <p>Additional Customization Options:</p> <ul> <li> <p>If you want specific chart types (bar charts, pie charts, network diagrams), specify those</p> </li> <li> <p>If you have a preferred color scheme or branding, mention it</p> </li> <li> <p>If you need the document in multiple formats, request that</p> </li> <li> <p>If you want interactive elements (for digital viewing), specify that requirement</p> </li> </ul> <p>This prompt will help create a comprehensive, visually appealing document that makes your complex lab infrastructure easy to understand and transition to your successor.</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the <code>sync_data_from_s3</code> command, for example), and then how to make the cleaned, final data sets.</p>"},{"location":"llm_info/","title":"LLM Information","text":""},{"location":"llm_info/#github","title":"Github","text":"<p>Copilot Claude is probably the best for coding right now.\u00a0 That may change.</p> <p>Your first custom instructions - GitHub Docs</p> <p>Awesome-copilot</p> <p>github/awesome-copilot: Community-contributed instructions, prompts, and configurations to help you make the most of GitHub Copilot.</p> <p>Beastmode in copilot</p> <p>awesome-copilot/chatmodes/Thinking-Beast-Mode.chatmode.md at main \u00b7 github/awesome-copilot</p>"},{"location":"llm_info/#youtube-sites","title":"Youtube sites","text":""},{"location":"llm_info/#this-is-more-advanced-natebjones","title":"This is more advanced NateBJones.","text":"<p>Nate Jones \u2014 Personal Site</p>"},{"location":"llm_info/#ai-news-strategy-daily","title":"Ai News &amp; Strategy Daily","text":""},{"location":"llm_info/#wes-roth-for-basic-updates","title":"Wes Roth:\u00a0 For basic updates","text":""},{"location":"llm_info/#arjancodes-python","title":"ArjanCodes:\u00a0 Python","text":""},{"location":"llm_info/#modern-software-engineering-youtube-very-experienced-software-engineer","title":"Modern Software Engineering.\u00a0 Youtube.\u00a0 Very experienced software engineer","text":""},{"location":"llm_info/#riffomonas-project-umich-does-mostly-r-work-when-i-first-started-out-using-the-cluster-this-was-useful","title":"Riffomonas Project:\u00a0 Umich, does mostly R work.\u00a0 When I first started out using the cluster this was useful.","text":"<p>https://youtu.be/vD0E3EUb8-8?si=yWVceKPwonDQKhQL</p>"},{"location":"llm_info/#aadl","title":"AADL","text":"<p>https://aadl.org/node/641052</p>"},{"location":"llm_info/#environment-management","title":"Environment management","text":"<p>I use Conda/miniconda.</p> <p>Started using UV due to increased popularity</p> <p>I don't do much with containerization with docker.</p> <p>Notes</p> <p>Confluence</p> <p>Confluence | Your Remote-Friendly Team Workspace | Atlassian</p> <p>Workflows</p> <p>Snakemake:\u00a0 Python based</p> <p>Home - Snakemake 8.0.0 documentation</p> <p>Johannes Koster developed snakemake and a lot of other bioinformatics sofware</p> <p>Misc links Generating Structured Outputs from LLMs | Towards Data Science</p>"},{"location":"mercury_shell_scripting/","title":"Mercury Shell Scripting","text":"<p>Hello - made some changes - commit 2</p> <p>## Housekeeping information</p>"},{"location":"mercury_shell_scripting/#all-dti-data-are-located-in-the-following-location-in-mercury-server","title":"All DTI data are located in the following location in Mercury Server.","text":"<pre><code>nfs/corenfs/psych-mercury-data/Data/DTI/SubjectID\n</code></pre>"},{"location":"mercury_shell_scripting/#subjectid-provides-information-about-the-cohort-and-visit","title":"SubjectID provides information about the cohort and visit.","text":"<pre><code>NF102_2\n</code></pre> Notation meaning N Non-stuttering F Female 10X Cohort 1 SubjectID 102 Visit Number 2"},{"location":"mercury_shell_scripting/#each-participant-data-folder-contains-the-following-files","title":"Each participant data folder contains the following files.","text":"Description Files anatomical images adc.nii, ad.nii, anat.nii, anat_orig.nii.gz Output folders from Freesurfer report, misc, surf, mri Secondary outputs fa.nii, rd.nii Normalized secondary outputs wadc.nii, wad.nii, wanat.nii, wb0.nii, wfa.nii, wFA_skeleton.nii,rd.nii Other b0_brain_mask.nii.gz, b0_brain.nii.gz, b0.nii, data.nii DTI data dti.nii Eddy correction outputs tmp.eddy.nii.eddy_command_txt, tmp.eddy.nii.eddy_movement_rms, tmp.eddy.nii.eddy_outlier_map, tmp.eddy.nii.eddy_outlier_n_sqr_stdev_map, tmp.eddy.nii.eddy_outlier_n_stdev_map, tmp.eddy.nii.eddy_outlier_report, tmp.eddy.nii.eddy_parameters, tmp.eddy.nii.eddy_post_eddy_shell_alignment_parameters, tmp.eddy.nii.eddy_post_eddy_shell_PE_translation_parameters, tmp.eddy.nii.eddy_restricted_movement_rms, tmp.eddy.nii.eddy_rotated_bvecs, tmp.eddy.nii.eddy_values_of_all_input_parameters, tmp.eddy.nii.gz"},{"location":"mercury_shell_scripting/#dti-csd-workflow","title":"DTI CSD workflow","text":""},{"location":"mercury_shell_scripting/#load-all-required-modules","title":"Load all required modules","text":"<pre><code>module load mrtrix\nmodule load fsl\nmodule load ANTs\nmodule load freesurfer\n</code></pre>"},{"location":"mercury_shell_scripting/#preprocessing-steps","title":"Preprocessing Steps","text":""},{"location":"mercury_shell_scripting/#step-1-file-type-conversion","title":"Step 1 : File type conversion","text":"<p>*Note: there is a common <code>bval</code> and <code>bvec</code> file set for each cohort. Copied that from subject folder level to each subject folder. </p> <pre><code>DTISourceFolder=\"/nfs/corenfs/psych-mercury-data/Data/DTI\"\n\ncp $DTISourceFolder/bvals Cohort1SubjectID/CohortSubjectID.bvals\ncp $DTISourceFolder/bvecs Cohort1SubjectID/CohortSubjectID.bvec\n</code></pre> <p>Combine raw diffusion data with <code>bval</code> and <code>bvec</code> files to use in future preprocessing steps.</p> <pre><code>mrconvert dti.nii SubID_dwi.mif -fslgrad SubID.bvecs SubID.bvals\n</code></pre> <p>Checking generated MRtrix Image file (<code>mif</code>)  --&gt; Quality Check 1 </p> <pre><code>mrinfo SubID_dwi.mif\n</code></pre> <p>The output contains several pieces of information, such as the dimensions of the dataset and the voxel size, along with the commands that were used to generate the current file. Note that, since this is a 4-dimensional dataset, the last dimension is\u00a0time*; in other words, this file contains 26 volumes, each one with dimensions of 128x128x48 voxels. The last dimension of the\u00a0<code>Voxel\u00a0size</code>\u00a0field - which in this case has a value of 13.7 - indicates the time it took to acquire each volume. This time is also called the repetition time, or TR. </p> <p>![[Pasted image 20240419105219.png]]</p> <pre><code>mrinfo -size SubID_dwi.mif | awk '{print $4}'\n</code></pre> <p>*Note: The number in the 4th field of the dimensions header that corresponds to the number of time-points, or volumes, in the dataset. We then compare this with the number of bvals and bvecs by using awk to count the number of columns in each text file:</p> <pre><code>awk '{print NF; exit}' SubID.bvecs\nawk '{print NF; exit}' SubID.bvals\ncat SubID.bvals\ncat SubID.bvecs\n</code></pre>"},{"location":"mercury_shell_scripting/#step-2-initial-preprocessing-noise-removal","title":"Step 2: Initial preprocessing (noise removal)","text":"<pre><code>dwidenoise SubID_dwi.mif SubID_dwi_den.mif -noise SubID_dwi_noise.mif \nmrcalc SubID_dwi.mif SubID_dwi_den.mif -subtract SubID_dwi_residual.mif\n</code></pre> <p>Checking generated MRtrix Image file (<code>mif</code>)  --&gt; Quality Check 2 </p> <p>*One quality check is to see whether the residuals load onto any part of the anatomy. If they do, that may indicate that the brain region is disproportionately affected by some kind of artifact or distortion. </p> <pre><code>mrview subID_dwi_residual.mif\n</code></pre> <p>![[Pasted image 20240418173006.png]] *It is common to see a grey outline of the brain, as in the figure above. However, everything within the grey matter and white matter should be relatively uniform and blurry; if you see any clear anatomical landmarks, such as individual gyri or sulci, that may indicate that those parts of the brain have been corrupted by noise. If that happens, you can increase the extent of the denoising filter from the default of 5 to a larger number, such as 7; e.g.,</p> <pre><code>dwidenoise your_data.mif your_data_denoised_7extent.mif -extent 7 -noise noise.mif\n</code></pre> <ul> <li>NOTE: We are not doing Gibbs ringing artifact removal (It was not a general issue for this dataset) OR Extracting the Reverse Phase-Encoded Images (because we do not have two encoding directions for this dataset)</li> </ul>"},{"location":"mercury_shell_scripting/#step-3-putting-it-all-together-preprocessing-with-dwipreproc","title":"Step 3: Putting it all together --&gt; preprocessing with dwipreproc","text":"<p>FSL commands are run so we will need to load module FSL</p> <pre><code>module load fsl\ndwifslpreproc subID_dwi_den.mif subID_dwi_den_preproc.mif -nocleanup -pe_dir PA -rpe_none -eddy_options \" --slm=linear\"\n</code></pre> <p>The first arguments are the input and output; the second option,\u00a0<code>-nocleanup</code>, will keep the temporary processing folder which contains a few files we will examine later.\u00a0<code>-pe_dir\u00a0AP</code>\u00a0signalizes that the primary phase-encoding direction is anterior-to-posterior, and\u00a0<code>-rpe_pair</code>\u00a0combined with the\u00a0<code>-se_epi</code>\u00a0options indicates that the following input file (i.e., \u201cb0_pair.mif\u201d) is a pair of spin-echo images that were acquired with reverse phase-encoding directions. Lastly,\u00a0<code>-eddy_options</code>\u00a0specifies options that are specific to the FSL command\u00a0<code>eddy</code>. You can visit the\u00a0eddy user guide\u00a0for more options and details about what they do. For now, we will only use the options\u00a0<code>--slm=linear</code>\u00a0(which can be useful for data that was acquired with less than 60 directions) and\u00a0<code>--data_is_shelled</code>\u00a0(which indicates that the diffusion data was acquired with multiple b-values).</p> <p>This command can takes roughly 2 hours. When it has finished, examine the output to see how eddy current correction and unwarping have changed the data; ideally, you should see more signal restored in regions such as the orbitofrontal cortex, which is particularly susceptible to signal dropout:</p> <pre><code>mrview subID_dwi_den_preproc.mif -overlay.load SubID_dwi.mif\n</code></pre> <p>This command will display the newly preprocessed data, with the original diffusion data overlaid on top of it and colored in red. To see how the eddy currents were unwarped, open the Overlays tab and click on the box next to the image\u00a0<code>subID_dwi.mif</code>. You should see a noticeable difference between the two images, especially in the frontal lobes of the brain near the eyes, which are most susceptible to eddy currents.</p>"},{"location":"mercury_shell_scripting/#step-4-checking-for-corrupt-slices-quality-check-3","title":"Step 4: Checking for corrupt slices  --&gt; Quality Check 3","text":"<p>One of the options in the\u00a0<code>dwifslpreproc</code>\u00a0command, \u201c-nocleanup\u201d, retained a directory with the string \u201ctmp\u201d in its title. Within this folder is a file called\u00a0<code>dwi_post_eddy.eddy_outlier_map</code>, which contains strings of 0\u2019s and 1\u2019s. Each 1 represents a slice that is an outlier, either because of too much motion, eddy currents, or something else.</p> <p>The following code, run from the\u00a0<code>dwi</code>\u00a0directory, will navigate into the \u201ctmp\u201d folder and calculate the percentage of outlier slices:</p> <pre><code>cd dwifslpreproc-tmp-*\ntotalSlices=`mrinfo dwi.mif | grep Dimensions | awk '{print $6 * $8}'`\ntotalOutliers=`awk '{ for(i=1;i&lt;=NF;i++)sum+=$i } END { print sum }' dwi_post_eddy.eddy_outlier_map`\necho \"If the following number is greater than 10, you may have to discard this subject because of too much motion or corrupted slices\"\necho \"scale=5; ($totalOutliers / $totalSlices * 100)/1\" | bc | tee percentageOutliers.txt\ncd ..\n</code></pre>"},{"location":"mercury_shell_scripting/#step-5-generating-a-mask","title":"Step 5: Generating a Mask","text":"<p>Create a mask to restrict your analysis only to brain voxels; this will speed up the rest of your analyses.</p> <p>To do that, it can be useful to run a command beforehand called\u00a0<code>dwibiascorrect</code>. This can remove inhomogeneities detected in the data that can lead to a better mask estimation. However, it can in some cases lead to a worse estimation; as with all of the preprocessing steps, you should check it before and after each step:</p> <pre><code>module load ANTs\ndwibiascorrect ants subID_dwi_den_preproc.mif subID_den_preproc_unbiased.mif -bias bias.mif\n</code></pre> <p>You are now ready to create the mask with\u00a0<code>dwi2mask</code>, which will restrict your analysis to voxels that are located within the brain:</p> <pre><code>dwi2mask subID_den_preproc_unbiased.mif mask.mif\n</code></pre> <p>OR force overwrite mask.mif that already exist</p> <pre><code>dwi2mask -force subID_den_preproc_unbiased.mif mask.mif\n</code></pre> <p>Check the output of this command by typing:</p> <pre><code>mrview mask.mif\n</code></pre> <p>Make sure the mask does not have any holes! Fix the holes with following commands, if there are any.</p> <pre><code>mrconvert subID_den_preproc_unbiased.mif subID_unbiased.nii\nbet2 subID_unbiased.nii subID_masked -m -f 0.2\nmrconvert subID_masked_mask.nii.gz mask_new.mif\n</code></pre>"},{"location":"mercury_shell_scripting/#constrained-spherical-deconvolution-analysis","title":"Constrained Spherical Deconvolution Analysis","text":""},{"location":"mercury_shell_scripting/#step-1-fiber-orientation-distribution","title":"Step 1 : Fiber Orientation Distribution","text":""},{"location":"mercury_shell_scripting/#response-function-estimation-estimate-different-response-functions-for-different-tissue-types-wm-gm-csf","title":"Response function estimation : Estimate different response functions for different tissue types (WM, GM, CSF)","text":"<p>In order to generate streamlines, first we need to estimate the orientation of fiber(s) in each voxel. We do this using Constrained Spherical Deconvolution (CSD; Tournier et al., 2004, 2007), instead with the tensor model, which was shown to outperform the performance of DTI in regions of crossing/kissing fibers (Farquharson et al., 2013). To perform CSD&lt; response function (RF) is necessary, which is used as a kernal for deconvolution. </p> <p>As many voxels contain both white and grey matter, or white matter and CSF (partial volumes), CSD is flawed in such voxels. We can improve our results by estimating different RFs for different tissue types. (i.e., The RF in white matter models the signal which is expected if there was only a fiber bundle with one coherent orientation present in a voxel). </p> <p>The RF generation is best done with DW data with different b-values as different b values are sensitive to different tissue types. This idea is at the core of multi-shell multi-tissue CSD (MSMT; Jeurissen et al., 2014). Our dataset has two b values (0 and 1000). Thus we can attempt this. However, as there are only two b values, we are only going to get RF for two tissue types (WM, and GM). This is a limitation as we do not model CSF well.</p> <p>However, the code requires that a csf output is provided --&gt; thus we left  the csf.txt output in the command. </p> <pre><code>dwi2response dhollander subID_dwi_den_preproc.mif -voxels voxels.mif wm.txt gm.txt csf.txt\n</code></pre> <p>This dataset can be viewed by typing the following:</p> <pre><code>mrview subID_dwi_den_preproc_unbiased.mif -overlay.load voxels.mif\n</code></pre> <p>The output from the\u00a0<code>dwi2response</code>\u00a0command, showing which voxels were used to construct a basis function for each tissue type. Red: CSF voxels; Green: Grey Matter voxels; Blue: White Matter voxels. Make sure that these colors are located where they should be; for example, the red voxels should be within the ventricles.</p> <p>You can then check the response function for each tissue type by typing:</p> <pre><code>shview wm.txt\nshview gm.txt\nshview csf.txt\n</code></pre> <p>Now we are ready do run the FOD analysis</p> <pre><code>dwi2fod msmt_csd subID_den_preproc.mif -mask mask.mif wm.txt wmfod.mif gm.txt gmfod.mif csf.txt csffod.mif\nmrconvert -coord 3 0 wmfod.mif - | mrcat csffod.mif gmfod.mif - vf.mif\nmrview vf.mif -odf.load_sh wmfod.mif\n</code></pre> <p>Also trying single shell approach --&gt; This commands takes a while at the 3 iterations. SO be patient.</p> <pre><code>module load mrtrix/tissue\n\nss3t_csd_beta1 sub-01_den_preproc_unbiased.mif wm.txt wmfod.mif gm.txt gmfod.mif csf.txt csffod.mif -mask mask.mif\n</code></pre> <p>Example: </p> <pre><code>ss3t_csd_beta1 NF130_1_den_preproc_unbiased.mif wm.txt wmfod_ss3t.mif gm.txt gmfod_ss3t.mif csf.txt csffod_ss3t.mif -mask mask.mif\n\nmrconvert -coord 3 0 wmfod_ss3t.mif - | mrcat csffod_ss3t.mif gmfod_ss3t.mif - vf_ss3t.mif\n\n\nmrview vf_ss3t.mif -odf.load_sh wmfod_ss3t.mif\n</code></pre> <p>SS3T version is only slightly different from MSMT approach (but you can see more green - csf areas)--&gt; MSMT approach with CSF tissue ditched is very sparse and possibly leads to reduced number of fiber tracks at streamline step we noted earlier. Here is a comparison. SS3T vs MSMT (without CSF) vs MSMT (with CSF)</p> <p>![[Pasted image 20240419152546.png]]</p>"},{"location":"mercury_shell_scripting/#normalization","title":"Normalization","text":"<p>Later on, for group-level analysis with the data that has been generated for each subject., we will need to\u00a0normalize\u00a0the FODs. This ensures that any differences we see are not due to intensity differences in the image, similar to how we correct for the size of the brain when comparing volumetric differences across subjects.</p> <p>To normalize the data, we will use the\u00a0<code>mtnormalise</code>\u00a0command. This requires an input and output for each tissue type, as well as a mask to restrict the analysis to brain voxels:</p> <pre><code>mtnormalise wmfod.mif wmfod_norm.mif gmfod.mif gmfod_norm.mif csffod.mif csffod_norm.mif -mask mask.mif\n</code></pre> <p>Noted that we get an error when trying with MSMT approach with csf.txt</p> <p>![[Pasted image 20240419152000.png]] Thus, switching to SS3T method output FODs:</p> <pre><code>mtnormalise wmfod_ss3t.mif wmfod__ss3t_norm.mif gmfod_ss3t.mif gmfod_ss3t_norm.mif csffod_ss3t.mif csffod_ss3t_norm.mif -mask mask.mif\n</code></pre>"},{"location":"mercury_shell_scripting/#generating-tissue-boundaries","title":"Generating Tissue boundaries","text":"<p>For streamline analysis, seeds\u00a0will be placed at random locations along the boundary between the grey matter and the white matter. A streamline will grow from each seed and trace a path from that seed region until it terminates in another region. Some of the streamlines will terminate in places that don\u2019t make sense - for example, a streamline may terminate at the border of the ventricles. We will cull these \u201cerror\u201d streamlines, and be left with a majority of streamlines that appear to connect distant grey matter regions.</p> <p>To do this, we will first need to create a\u00a0boundary\u00a0between the grey matter and the white matter. The MRtrix command\u00a0<code>5ttgen</code>\u00a0will use FSL\u2019s FAST, along with other commands, to segment the anatomical image into five tissue types:</p> <pre><code>1. Grey Matter;\n2. Subcortical Grey Matter (such as the amygdala and basal ganglia);\n3. White Matter;\n4. Cerebrospinal Fluid; and\n5. Pathological Tissue.\n</code></pre> <p>Once we have segmented the brain into those tissue classes, we can then use the boundary as a mask to restrict where we will place our seeds.</p>"},{"location":"mercury_shell_scripting/#step-1-convert-anatomical-image-to-mrtrix-format","title":"Step 1 : Convert anatomical image to MRtrix format","text":"<p>the anatomical image is present in each subject folder as <code>anat_orig.nii.gz</code> --&gt; we will make a copy as <code>T1.ni.gz</code> --&gt; unzip it (<code>TI.nii</code>) and convert it to <code>T1.mif</code> </p> <pre><code>cp anat_orig.nii.gz T1.nii.gz\ngzip -d T1.nii.gz\nmrconvert T1.nii T1.mif\n</code></pre>"},{"location":"mercury_shell_scripting/#step-2-segmenting-to-tissue-types","title":"Step 2: Segmenting to tissue types","text":"<p>We will now use the command\u00a0<code>5ttgen</code>\u00a0to segment the anatomical image into the tissue types listed above: (~15 mins)</p> <pre><code>5ttgen fsl T1.mif 5tt_nocoreg.mif\nmrview 5tt_nocoreg.mif\n</code></pre> <p>If the segmentation has finished successfully, you should see the following images when you type\u00a0<code>mrview\u00a05tt_nocoreg.mif</code>\u00a0(pressing the left and right arrow keys scrolls through the different tissue types): We only see four types - not five. The final volume will be blank as we do not have pathological Tissue.</p> <p>![[Pasted image 20240419161022.png]]</p>"},{"location":"mercury_shell_scripting/#co-registering-the-diffusion-and-anatomical-images","title":"Co-registering the Diffusion and Anatomical Images","text":"<p>Next step is to co-register the anatomical and diffusion-weighted images. This ensures that the boundaries of the tissue types are aligned with the boundaries of the diffusion-weighted images; even small differences in the location of the two scans can throw off the tractography results.</p> <p>We will first use the commands\u00a0<code>dwiextract</code>\u00a0and\u00a0<code>mrmath</code>\u00a0to average together the B0 images from the diffusion data. These are the images that look most like T2-weighted functional scans, since a diffusion gradient wasn\u2019t applied during their acquisition - in other words, they were acquired with a b-value of zero. </p> <pre><code>dwiextract subID_den_preproc_unbiased.mif - -bzero | mrmath - mean mean_b0.mif -axis 3\n</code></pre> <p>There are two parts to this command, separated by a pipe (\u201d<code>|</code>\u201d). The left half of the command,\u00a0<code>dwiextract</code>, takes the preprocessed diffusion-weighted image as an input, and the\u00a0<code>-bzero</code>\u00a0option extracts the B0 images; the solitary\u00a0<code>-</code>\u00a0argument indicates that the output should be used as input for the second part of the command, to the right of the pipe.\u00a0<code>mrmath</code>\u00a0then takes these output B0 images and computes the mean along the 3rd axis, or the time dimension. In other words, if we start with an index of 0, then the number 3 indicates the 4th dimension, which simply means to average over all of the volumes.</p> <p>In order to carry out the coregistration between the diffusion and anatomical images, we will need to take a brief detour outside of MRtrix. The software package doesn\u2019t have a coregistration command in its library, so we will need to use another software package\u2019s commands instead. Although you can choose any one you want, we will focus here on FSL\u2019s\u00a0<code>flirt</code>\u00a0command.</p> <p>The first step is to convert both the segmented anatomical image and the B0 images we just extracted:</p> <pre><code>mrconvert mean_b0.mif mean_b0.nii.gz\nmrconvert 5tt_nocoreg.mif 5tt_nocoreg.nii.gz\n</code></pre> <p>Since\u00a0<code>flirt</code>\u00a0can only work with a single 3D image (not 4D datasets), we will use\u00a0<code>fslroi</code>\u00a0to extract the first volume of the segmented dataset, which corresponds to the Grey Matter segmentation:</p> <pre><code>fslroi 5tt_nocoreg.nii.gz 5tt_vol0.nii.gz 0 1\n</code></pre> <p>We then use the\u00a0<code>flirt</code>\u00a0command to coregister the two datasets:</p> <pre><code>flirt -in mean_b0.nii.gz -ref 5tt_vol0.nii.gz -interp nearestneighbour -dof 6 -omat diff2struct_fsl.mat\n</code></pre> <p>This command uses the grey matter segmentation (i.e., \u201c5tt_vol0.nii.gz\u201d) as the reference image, meaning that it stays stationary. The averaged B0 images are then moved to find the best fit with the grey matter segmentation. The output of this command, \u201cdiff2struct_fsl.mat\u201d, contains the\u00a0transformation matrix\u00a0that was used to overlay the diffusion image on top of the grey matter segmentation.</p> <p>Now that we have generated our transformation matrix, we will need to convert it into a format that can be read by MRtrix. That is, we are now ready to travel back into MRtrix after briefly stepping outside of it. The command\u00a0<code>transformconvert</code>\u00a0does this:</p> <pre><code>transformconvert diff2struct_fsl.mat mean_b0.nii.gz 5tt_nocoreg.nii.gz flirt_import diff2struct_mrtrix.txt\n</code></pre> <p>Note that the above steps used the anatomical segmentation as the reference image. We did this because usually the coregistration is more accurate if the reference image has higher spatial resolution and sharper distinction between the tissue types. However, we also want to introduce as few edits and interpolations to the functional data as possible during preprocessing. Therefore, since we already have the steps to transform the diffusion image to the anatomical image, we can take the inverse of the transformation matrix to do the opposite - i.e., coregister the anatomical image to the diffusion image:</p> <pre><code>mrtransform 5tt_nocoreg.mif -linear diff2struct_mrtrix.txt -inverse 5tt_coreg.mif\n</code></pre> <p>The resulting file, \u201c5tt_coreg.mif\u201d, can be loaded into\u00a0<code>mrview</code>\u00a0in order to examine the quality of the coregistration:</p> <pre><code>mrview subID_den_preproc_unbiased.mif -overlay.load 5tt_nocoreg.mif -overlay.colourmap 2 -overlay.load 5tt_coreg.mif -overlay.colourmap 1\n</code></pre> <p>The \u201coverlay.colourmap\u201d options specify different color codes for each image that is loaded. In this case, the boundaries before coregistration will be depicted in blue, and the boundaries after coregistration will be shown in red. The change might be slight but makes a big differences for later analysis steps.</p>"},{"location":"mercury_shell_scripting/#create-seed-boundaries","title":"Create seed boundaries","text":"<p>The last step to create the \u201cseed\u201d boundary - the boundary separating the grey from the white matter, which we will use to create the seeds for our streamlines - is created with the command\u00a0<code>5tt2gmwmi</code>\u00a0(which stands for \u201c5 Tissue Type (segmentation) to Grey Matter / White Matter Interface)</p> <pre><code>5tt2gmwmi 5tt_coreg.mif gmwmSeed_coreg.mif\n</code></pre> <p>Again, we will check the result with\u00a0<code>mrview</code>\u00a0to make sure the interface is where we think it should be:</p> <pre><code>mrview subID_den_preproc_unbiased.mif -overlay.load gmwmSeed_coreg.mif\n</code></pre>"},{"location":"mercury_shell_scripting/#streamlines","title":"Streamlines","text":""},{"location":"mercury_shell_scripting/#anatomically-constrained-tractography","title":"Anatomically Constrained Tractography","text":"<p>One of MRtrix\u2019s features is\u00a0Anatomically Constrained Tractography, or ACT. This method will only determine that a streamline is valid if it is biologically plausible. For example, a streamline that terminates in the cerebrospinal fluid will be discarded, since white matter tracts tend to both originate and terminate in grey matter. In other words, the streamlines will be constrained to the white matter. Anatomically constrained tractography isn\u2019t a separate preprocessing step, but rather an option that can be included with the command\u00a0<code>tckgen</code>, which generates the actual streamlines.</p>"},{"location":"mercury_shell_scripting/#generating-streamlines-with-tckgen","title":"Generating Streamlines with tckgen","text":"<p>MRtrix is able to do both\u00a0deterministic\u00a0and\u00a0probabilistic\u00a0tractography. In deterministic tractography, the direction of the streamline at each voxel is determined based on the predominant fiber orientation; in other words, the streamline is determined by a single parameter. MRtrix includes multiple options to do this type of deterministic tractography, such as\u00a0<code>FACT</code>\u00a0or\u00a0<code>tensor_det</code>.</p> <p>The other method, probabilistic tractography, is the default in MRtrix. In this approach, multiple streamlines are generated from seed regions all along the boundary between the grey matter and white matter. The direction of the streamline will most likely follow the predominant fiber orientation density, but not always; due to a large number of samples, some streamlines will follow other directions. This becomes less likely if the FOD is extremely strong in one direction - for example, the FODs within a structure such as the corpus callosum will tend to all be aligned left-to-right - but the sampling becomes more diverse in regions that do not have a predominant fiber orientation.</p> <p>The default method is to use an algorithm known as iFOD2, which will use a probabilistic streamline approach. Other algorithms can be found at\u00a0this site. We will use the default of iFOD2.</p>"},{"location":"mercury_shell_scripting/#how-many-streamlines","title":"How Many Streamlines?","text":"<p>There is a trade-off between the number of generated streamlines and the amount of time that it takes. More streamlines result in a more accurate reconstruction of the underlying white-matter tracts, but estimating a large number of them can take a prohibitively long time.</p> <p>The \u201ccorrect\u201d number of streamlines to use is still being debated, but at least 10 million or so should be a good starting place:</p> <pre><code>tckgen -act 5tt_coreg.mif -backtrack -seed_gmwmi gmwmSeed_coreg.mif -nthreads 8 -maxlength 250 -cutoff 0.06 -select 10000000 wmfod_ss3t_norm.mif tracks_10M.tck\n</code></pre> <p>In this command, </p> -act specifies that we will use the anatomically-segmented image to constrain our analysis to the white matter. -backtrack indicates for the current streamline to go back and run the same streamline again if it terminates in a strange place (e.g., the cerebrospinal fluid). -maxlength sets the maximum tract length, in voxels, that will be permitted; and \u201c-cutoff\u201d specifies the FOD amplitude for terminating a tract (for example, a value of 0.06 would not permit a streamline to go along an FOD that is lower than that number). -seed_gmwmi takes as an input the grey-matter / white-matter boundary that was generated using the\u00a0<code>5tt2gmwmi</code>\u00a0command. -nthreads specifies the number of processing cores you wish to use, in order to speed up the analysis. -select indicates how many total streamlines to generate. Note that a shorthand can be used if you like; instead of, say, 10000000, you can rewrite it as 10000k (meaning \u201cten thousand thousands\u201d, which equals \u201cten million\u201d). last two arguments The last two arguments specify both the input (<code>wmfod_norm.mif</code>) and a label for the output (<code>tracks_10M.tck</code>). <p>If you want to visualize the output, extract a subset of the output by using\u00a0<code>tckedit</code>:</p> <pre><code>tckedit tracks_10M.tck -number 200k smallerTracks_200k.tck\n</code></pre> <p>This can then be loaded into\u00a0<code>mrview</code>\u00a0by using the \u201c-tractography.load\u201d option, which will automatically overlay the smallerTracks_200k.tck file onto the preprocessed diffusion-weighted image:</p> <pre><code>mrview subID_den_preproc_unbiased.mif -tractography.load smallerTracks_200k.tck\n</code></pre> <p>The streamlines should be constrained to the white matter, and they should be color-coded appropriately. For example, the corpus callosum should be mostly red, and the corona radiata should be mostly blue.</p>"},{"location":"mercury_shell_scripting/#refining-the-streamlines","title":"Refining the streamlines","text":"<p>Although we have created a diffusion image with reasonable streamlines, also known as a\u00a0tractogram, we still have a problem with some of the white matter tracts being over-fitted, and others being under-fitted. This can be addressed with the\u00a0<code>tcksift2</code>\u00a0command.</p> <p>The reason is that some tracts will be threaded with more streamlines than others, because the fiber orientation densities are much clearer and more attractive candidates for the probabilistic sampling algorithm that was discussed above. In other words, certain tracts can be over-represented by the amount of streamlines that pass through them not necessarily because they contain more fibers, but because the fibers tend to all be orientated in the same direction.</p> <p>To counter-balance this overfitting, the command\u00a0<code>tcksift2</code>\u00a0will create a text file containing weights for each voxel in the brain:</p> <pre><code>tcksift2 -act 5tt_coreg.mif -out_mu sift_mu.txt -out_coeffs sift_coeffs.txt -nthreads 8 tracks_10M.tck wmfod_ss3t_norm.mif sift_1M.txt\n</code></pre> <p>The output from the command, \u201csift_1M.txt\u201d, can be used with the command\u00a0<code>tck2connectome</code>\u00a0to create a matrix of how much each ROI is connected with every other ROI in the brain - a figure known as a\u00a0connectome\u00a0- which will weight each ROI. </p>"},{"location":"mercury_shell_scripting/#creating-and-viewing-the-connectome","title":"Creating and Viewing the Connectome","text":"<p>We can create a\u00a0connectome\u00a0that represents the number of streamlines connecting different parts of the brain. To do that, we have to first parcellate the brain into different regions, or nodes. One way to do this is by using an\u00a0atlas, which assigns each voxel in the brain to a specific ROI.</p> <p>We will be using the atlases that come with\u00a0FreeSurfer. Accordingly, our first step will be to run the subject\u2019s anatomical image through recon-all, which you can read more about\u00a0here:</p> <pre><code>module load freesurfer\nrecon-all -i anat_orig.nii.gz -s subID_recon -all\n</code></pre> <p>We can also use the T1.nii we generated earlier through anat-Orign.nii.gz for this step instead.</p> <pre><code>recon-all -i T1.nii -s subID_recon -all\n</code></pre> <p>This will take a few hours (4.425 hours!), depending on the speed of your computer. When it has finished, make sure to check the output by using the QA procedures described in\u00a0this chapter.</p>"},{"location":"mercury_shell_scripting/#creating-the-connectome","title":"Creating the Connectome","text":"<p>When recon-all has finished, we will need to convert the labels of the FreeSurfer parcellation to a format that MRtrix understands. The command\u00a0<code>labelconvert</code>\u00a0will use the parcellation and segmentation output of FreeSurfer to create a new parcellated file in .mif format:</p> <p>Define freesurfer and mrtrix_home path to make the code generic.</p> <p>$FREESURFER_HOME='/app/apps/rhel8/freesurfer/7.1.1' $MRTRIX_HOME='/app/apps/rhel8/mrtrix/3.0.4'</p> <pre><code>labelconvert ../subjects/subID_recon_all/mri/aparc+aseg.mgz $FREESURFER_HOME/FreeSurferColorLUT.txt $MRTRIX_HOME/share/mrtrix3/labelconvert/fs_default.txt subID_parcels.mif\n</code></pre> <p>We then need to create a whole-brain connectome, representing the streamlines between each parcellation pair in the atlas (in this case, 84x84). </p> symmetric option will make the lower diagonal the same as the upper diagonal scale_invnodevol option will scale the connectome by the inverse of the size of the node. <pre><code>tck2connectome -symmetric -zero_diagonal -scale_invnodevol -tck_weights_in sift_1M.txt tracks_1M.tck subID_parcels.mif subID_parcels.csv -out_assignment assignments_subID_parcels.csv\n</code></pre> <p>Used tracks_10M.tck ?? is that wrong ?? no tracks_1M.tck file available</p>"},{"location":"mercury_shell_scripting/#viewing-the-connectome","title":"Viewing the Connectome","text":"<p>Once you have created the\u00a0<code>parcels.csv</code>\u00a0file, you can view it as a matrix in <code>Matlab</code>. First, you will need to import it:</p> <pre><code>module load MATLAB/R2021a\n\nconnectome = importdata('NF130_1_parcels.csv');\n</code></pre> <p>And then you will need to view it as a scaled image, so that higher structural connectivity pairs are brighter:</p> <pre><code>imagesc(connectome);\n</code></pre> <p>![[Pasted image 20240420093515.png]]</p> <p>The most noticeable feature is a division of the figure into two distinct \u201cboxes\u201d, representing increased structural connectivity within each hemisphere. You will also observe a relatively brighter line traced along the diagonal, representing higher structural connectivity between nearby nodes. Brighter boxes in the opposing bottom-left and upper-right corners represent increased structural connectivity between homologous regions.</p> <p>To make these associations more obvious, you can change the scaling of the color map:</p> <pre><code>imagesc(connectome, [0 1]);\n</code></pre> <p>![[Pasted image 20240420093550.png]]</p>"},{"location":"mercury_shell_scripting/#generating-roi-based-tractography","title":"Generating ROI based tractography","text":"<p>To figure out the ROI look at fs_default.txt in MTRIX folder that was used in the label-convert step:</p> <p><code>cat $MRTRIX_HOME/share/mrtrix3/labelconvert/fs_default.txt</code></p> <p>For IFG:</p> <pre><code>connectome2tck -nodes 17 tracks_10M.tck assignments_NF130_1_parcels.csv -files per_node LeftParsopecularis\nmrview 5tt_coreg.mif -tractography.load LeftParsopecularis17.tck\n</code></pre> <p>Initially, using MSMT --&gt; here were reduced tract density and many tracts were missing</p> <p>![[Pasted image 20240421213931.png]]![[Pasted image 20240421214004.png]] ![[Pasted image 20240421214013.png]] Using SH3T --&gt; the results were much better (more fibers reaching the posterior auditory/parietal regions)</p> <p>1Mtracks</p> <p>![[Pasted image 20240422173655.png]] ![[Pasted image 20240422173637.png]]</p> <p>![[Pasted image 20240422173612.png]]</p> <p>10Mtracks ![[Pasted image 20240422173937.png]] ![[Pasted image 20240422173905.png]]</p> <p>![[Pasted image 20240422173845.png]]</p> <p>Go back and generate the 1M track version all the way from <code>tckgen</code> step if you want to use a different number of tracks for the process. For viewing, you can always reduce to 200k tracks using <code>tckedit</code>. </p> <pre><code>tckgen -act 5tt_coreg.mif -backtrack -seed_gmwmi gmwmSeed_coreg.mif -nthreads 8 -maxlength 250 -cutoff 0.06 -select 1000000 wmfod_ss3t.mif tracks_1M.tck\n\ntckedit tracks_1M.tck -number 200k smallerTracks_200k.tck\n\nmrview NF130_1_dwi_den_preproc.mif -tractography.load smallerTracks_200k.tck\n\ntcksift2 -act 5tt_coreg.mif -out_mu sift_mu.txt -out_coeffs sift_coeffs.txt -nthreads 8 tracks_1M.tck wmfod_norm.mif sift_1M.txt\n</code></pre> <pre><code>tck2connectome -symmetric -zero_diagonal -scale_invnodevol -tck_weights_in sift_1M.txt tracks_1M.tck NF101_1_parcels.mif NF101_1_parcels_1Mtrack.csv -out_assignment assignments_NF101_1_parcels_1Mtrack.csv\n</code></pre>"},{"location":"mercury_shell_scripting/#comparing-different-track-densities-and-single-shell-multi-shell-methods","title":"Comparing different track densities and single shell multi shell methods","text":"<p>MMMT</p> <p>Initially, using MSMT --&gt; here were reduced tract density and many tracts were missing</p> <p>1M tracks ![[Pasted image 20240421213931.png]]</p> <p>SS3T 1Mtracks</p> <p>![[Pasted image 20240422173655.png]]</p> <p>10Mtracks ![[Pasted image 20240422173937.png]]</p>"},{"location":"mercury_shell_scripting/#subcortical-segmentations","title":"Subcortical segmentations","text":"<p>Freesurfer V3 provides more detailed subcortical segmentations (run time 11 hours)</p> <p>https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferVersion3</p> <p>In automatic subcortical segmentation, each voxel in the normalized brain volume is assigned one of about 40 labels, including:</p> <ul> <li>Cerebral White Matter, Cerebral Cortex, Lateral Ventricle, Inferior Lateral Ventricle, Cerebellum\u00a0White Matter, Cerebellum Cortex, Thalamus, Caudate, Putamen, Pallidum, Hippocampus, Amygdala, Lesion,\u00a0Accumbens area, Vessel, Third Ventricle, Fourth Ventricle, Brain Stem, Cerebrospinal Fluid</li> </ul> <p>FreeSurfer\u00a0now runs automated labeling of the brain volume and this is included in the stable v3.0 release,\u00a0during the -autorecon2 stage. However, if you processed your anatomical data using previous versions and you wish to obtain the automated labels, you can just run the subcortical segmentation separately.</p> <p>To obtain automatically segmented volumes for the first time (assuming the\u00a0-autorecon1 stage\u00a0has completed), run:</p> <p>Make sure <code>$SUBJECT_DIR</code> is set accordingly  <code>SUBJECTS_DIR='/nfs/corenfs/psych-mercury-data/Data/DTI/subjects'</code> `recon-all -subcortseg -subjid ' <p><code>recon-all -subcortseg -subjid ../NF130_1_recon</code></p> <p>Ventrolateral thalamus --&gt; also check Frank's model For basal ganglia thalamo cortical loop </p>"},{"location":"mercury_shell_scripting/#generating-batch-scripts","title":"Generating batch scripts","text":"<p>For now, we will do this for a single subject. The scripts will be written in four parts:</p> <ol> <li> <p>The first script will perform all of the preprocessing, from denoising to\u00a0<code>tcksift2</code>;</p> </li> <li> <p>The second script will perform QA checks for each of the major preprocessing outputs;</p> </li> <li> <p>The third script will preprocess the structural images using\u00a0<code>recon-all</code>; and</p> </li> <li> <p>The last script will create the connectome.</p> </li> </ol> <p><code>recon-all</code>\u00a0isn\u2019t part of the MRtrix pipeline\u00a0per se\u00a0- you can use any atlas you want, and you are not restricted to FreeSurfer - but we will include it as a prerequisite for creating the connectome.</p> <p>[[batch script (Based on Andy's scripts) - modified]]</p> <pre><code>#!/bin/bash\n\nmrconvert dti.nii DTI.mif -fslgrad NF101_1.bvecs NF101_1.bvals\ndwidenoise DTI.mif DTI_den.mif -noise DTI_noise.mif\nmrcalc DTI.mif DTI_den.mif -subtract DTI_residual.mif\nmrview DTI_residual.mif\ndwifslpreproc DTI_den.mif DTI_den_preproc.mif -nocleanup -pe_dir PA -rpe_none -eddy_options \" --slm=linear\"\nmrview DTI_den_preproc.mif\ndwi2mask DTI_den_preproc.mif DTI_mask.mif\nmrview DTI_mask.mif \ndwi2response dhollander DTI_den_preproc.mif -voxels voxels.mif wm.txt gm.txt csf.txt\nshview wm.txt\nshview gm.txt\nshview csf.txt\ndwi2fod msmt_csd DTI_den_preproc.mif -mask DTI_mask.mif wm.txt wmfod.mif gm.txt gmfod.mif csf.txt csffod.mif\nmrconvert -coord 3 0 wmfod.mif - | mrcat csffod.mif gmfod.mif - vf.mif\nmrview vf.mif -odf.load_sh wmfod.mif\n\nmrconvert T1.nii T1.mif\n5ttgen fsl T1.mif 5tt_nocoreg.mif\nmrview 5tt_nocoreg.mif\n\ndwiextract DTI_den_preproc.mif - -bzero | mrmath - mean mean_b0.mif -axis 3\nmrconvert mean_b0.mif mean_b0.nii.gz\nmrconvert 5tt_nocoreg.mif 5tt_nocoreg.nii.gz\nfslroi 5tt_nocoreg.nii.gz 5tt_vol0.nii.gz 0 1\nflirt -in mean_b0.nii.gz -ref 5tt_vol0.nii.gz -interp nearestneighbour -dof 6 -omat diff2struct_fsl.mat\ntransformconvert diff2struct_fsl.mat mean_b0.nii.gz 5tt_nocoreg.nii.gz flirt_import diff2struct_mrtrix.txt\nmrtransform 5tt_nocoreg.mif -linear diff2struct_mrtrix.txt -inverse 5tt_coreg.mif\nmrview DTI_den_preproc.mif -overlay.load 5tt_nocoreg.mif -overlay.colourmap 2 -overlay.load 5tt_coreg.mif -overlay.colourmap 1\n5tt2gmwmi 5tt_coreg.mif gmwmSeed_coreg.mif\nmrview DTI_den_preproc.mif -overlay.load gmwmSeed_coreg.mif\n\ntckgen -act 5tt_coreg.mif -backtrack -seed_gmwmi gmwmSeed_coreg.mif -nthreads 8 -maxlength 250 -cutoff 0.06 -select 1000000 wmfod.mif tracks_1M.tck\ntckedit tracks_1M.tck -number 200k smallerTracks_200k.tck\nmrview DTI_den_preproc.mif -tractography.load smallerTracks_200k.tck\n\ntcksift2 -act 5tt_coreg.mif -out_mu sift_mu.txt -out_coeffs sift_coeffs.txt -nthreads 8 tracks_1M.tck wmfod_norm.mif sift_1M.txt\n\nrecon-all -i T1.nii -s sub-NF101_1_recon -all\n\nlabelconvert aparc+aseg.mgz $FREESURFER_HOME/FreeSurferColorLUT.txt /app/apps/rhel8/mrtrix/3.0.4/share/mrtrix3/labelconvert/fs_default.txt NF101_1_parcels.mif\n\ntck2connectome -symmetric -zero_diagonal -scale_invnodevol -tck_weights_in sift_1M.txt tracks_1M.tck NF101_1_parcels.mif NF101_1_parcels.csv -out_assignment assignments_NF101_1_parcels.csv\n</code></pre> <p>Matlab commands connectome = importdata('NF101_1_parcels.csv');</p>"},{"location":"mercury_shell_scripting/#code-to-select-rois-and-seeds-of-interest","title":"Code to select ROIs and seeds of interest","text":"<p>mrcalc NF101_1_parcels.mif 38 -eq L_PU.mif mrcalc NF101_1_parcels.mif 45 -eq R-PU.mif</p> <p>connectome2tck -nodes 38,45 -exclusive tracks_1M.tck assignments_NF101_1_parcels.csv moto mrview 5tt_coreg.mif -tractography.load moto38-45.tck</p> <p>connectome2tck -nodes 36,38 -exclusive tracks_1M.tck assignments_NF101_1_parcels.csv moto</p> <p>mrview 5tt_coreg.mif -tractography.load moto36-38.tck</p> <p>connectome2tck -nodes 37,23 tracks_1M.tck assignments_NF101_1_parcels.csv -files per_node LeftCaudate mrview 5tt_coreg.mif -tractography.load LeftCaudate23.tck    </p> <p>LeftCaudate37.tck  </p> <p>less $FREESURFER_HOME/FreeSurferColorLUT.txt</p> <p>freeview -v 004/mri/orig.mgz \\ 004/mri/aparc+aseg.mgz:colormap=lut:opacity=0.4 \\ -f 004/surf/lh.white:annot=aparc.annot</p> <p>freeview -v mri/orig.mgz \\ mri/aparc+aseg.mgz:colormap=lut:opacity=0.4 \\ -f surf/lh.white:annot=aparc.annot</p> <p>connectome2tck -nodes 17 tracks_1M.tck assignments_NF101_1_parcels.csv -files per_node LeftParsopecularis mrview 5tt_coreg.mif -tractography.load LeftParsopecularis17.tck    </p> <p>Generating stramlines that arandomly seeded from a mask ROI to the rest of the brain</p> <p>**  </p> <p>tckgen -seed_image mask_Left_Cluster1.mif tracks_1M.tck output_Left-Cluster1.tck</p> <p>**</p>"},{"location":"transition_plan/","title":"Transition Plan","text":"<p>GitHub Copilot: Thank you for providing both the prompt and the YAML file. Before I generate your transition guide, could you please clarify the following preferences:</p> <ol> <li>What is your preferred output format? (Word, PDF, Markdown, or multiple formats?)</li> <li>Do you have a preferred color scheme, branding, or logo to include?</li> <li>Would you like specific chart types (e.g., bar charts, network diagrams), or should I choose what best fits the data?</li> <li>Do you want interactive elements for digital viewing, or should the document be print-focused only?</li> <li>Should I use placeholder contact information, or do you want to provide real names/emails?</li> </ol> <p>Once you confirm these details, I\u2019ll proceed to create a comprehensive, visually organized transition document as specified.</p> <p>delpropo: I would like the information in markdown.  You can use mermaid for creating charts or plots so the information can easily be put on github.  Use placeholder contact information.  Format so that it can be viewed easily as a markdown file on github.</p> <p>GitHub Copilot: Thank you for the details! Here\u2019s my plan for your transition guide in Markdown, optimized for GitHub viewing:</p> <ul> <li>Professional title page (YAML metadata)</li> <li>Table of contents (with anchor links)</li> <li>Executive summary</li> <li>Logical sections with clear headings</li> <li>Tables for file/folder inventories and system comparisons</li> <li>Mermaid diagrams for architecture, flowcharts, and matrices</li> <li>Callout boxes using blockquotes and emoji/icons for visual cues</li> <li>Glossary, troubleshooting, contact placeholders, and next steps</li> </ul> <p>I\u2019ll generate the full Markdown document, including all required visual elements and formatting for GitHub. Let me know if you want any specific section order or additional customization before I proceed. Otherwise, I\u2019ll start creating the document now.</p> <p>delpropo: proceed</p> <p>GitHub Copilot: I am unable to create the Markdown file directly because the target location is outside the current workspace or not open in the editor. </p> <p>To proceed, please do one of the following: - Open your desired target folder as the workspace in VS Code, or - Open a new or existing Markdown file in your editor where you want the content to go.</p> <p>Once you do that, let me know, and I can generate the content directly into your open file. If you prefer, I can also provide the full Markdown content here for you to copy and paste. Let me know how you\u2019d like to proceed!</p> <p>delpropo: generate the information so I can paste it into a file</p> <p>GitHub Copilot: Here is your comprehensive transition guide in Markdown, ready to copy and paste into a file. All diagrams use Mermaid and formatting is optimized for GitHub.</p>"},{"location":"transition_plan/#mni-tempname-temp-lab-transition-guide","title":"MNI TEMPNAME TEMP Lab Transition Guide","text":"<p>Lab: mni-TEMPNAME-TEMP-lab Prepared for: Successor(s) Created by: Jim TEMP Created date: 8/22/25 Last updated: 8/22/25 Purpose: Documentation for position transition</p>"},{"location":"transition_plan/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Executive Summary</li> <li>Storage Architecture Overview</li> <li>File Location Flowchart</li> <li>Storage Systems &amp; Access Methods</li> <li>File &amp; Folder Inventory</li> <li>System Comparison: Great Lakes vs. Armis2</li> <li>Transition Priorities &amp; Matrix</li> <li>Timeline &amp; Handover Checklist</li> <li>Glossary</li> <li>Troubleshooting</li> <li>Contact Information</li> <li>Next Steps / Action Items</li> </ul>"},{"location":"transition_plan/#executive-summary","title":"Executive Summary","text":"<p> Critical: The lab's inventory files and Turbo storage are essential for ongoing operations. Immediate attention is needed for \"Work in progress\" items in OneDrive and R Drive documentation.</p> <p>This guide provides a comprehensive overview of the lab's digital resources, storage systems, and computing infrastructure. It is designed to ensure a smooth transition and continued success for the next lab manager or team member.</p>"},{"location":"transition_plan/#storage-architecture-overview","title":"Storage Architecture Overview","text":"<pre><code>graph TD\n    OneDrive[OneDrive]\n    RDrive[R Drive]\n    Turbo[Turbo Storage]\n    GreatLakes[Great Lakes Cluster]\n    Armis2[Armis2 Cluster]\n    Dataden[Dataden]\n\n    OneDrive -- Inventory, Docs --&gt; Turbo\n    RDrive -- Backups, Legacy --&gt; Turbo\n    Turbo -- Data Processing --&gt; GreatLakes\n    Turbo -- Sensitive Data --&gt; Armis2\n    Turbo -- Data Export --&gt; Dataden\n    Dataden -- API Access --&gt; OneDrive\n</code></pre>"},{"location":"transition_plan/#file-location-flowchart","title":"File Location Flowchart","text":"<pre><code>flowchart TD\n    A[Data Created/Received] --&gt; B{Where is it stored?}\n    B -- Inventory/Docs --&gt; C[OneDrive]\n    B -- Large Data/Analysis --&gt; D[Turbo Storage]\n    B -- Backups/Legacy --&gt; E[R Drive]\n    D --&gt; F{Which cluster?}\n    F -- General Research --&gt; G[Great Lakes]\n    F -- Sensitive Data --&gt; H[Armis2]\n    D --&gt; I[Dataden]\n</code></pre>"},{"location":"transition_plan/#storage-systems-access-methods","title":"Storage Systems &amp; Access Methods","text":"Storage System Location/Path Access Method Notes OneDrive OneDrive - Michigan Medicine/Documents - mni-TEMPNAME-TEMP-lab.mm Web, Desktop Sync Work in progress R Drive R:\\MM-MNI-TEMPLab Network Drive Work in progress Turbo Storage /nfs/turbo/umms-TEMPNAME HPC Cluster Mount Used on Great Lakes &amp; Armis2 Dataden umms-TEMPNAME.turbo.storage.umich.edu:/umms-TEMPNAME Globus For data sharing"},{"location":"transition_plan/#file-folder-inventory","title":"File &amp; Folder Inventory","text":""},{"location":"transition_plan/#inventory-files","title":"Inventory Files","text":"File Path Description Type Access Frequency Dependent Processes Notes Priority Documents - mni-TEMPNAME-TEMP-lab.mm/inventory/samples_and_purification.xlsx List of samples that have been purified Excel Routine Sample tracking, Purification protocols Critical for ongoing sample management. Some samples may have been pulled and are in lab and not -80C High Documents - mni-TEMPNAME-TEMP-lab.mm/inventory/Inventory.xlsx Complete inventory of all lab samples Excel Routine Sample location tracking, Inventory management Master inventory file - essential for lab operations High"},{"location":"transition_plan/#critical-directories","title":"Critical Directories","text":"Folder Path Description Contents Summary Notes Priority Documents - mni-TEMPNAME-TEMP-lab.mm/inventory/ Contains all inventory-related files Sample tracking, purification records, master inventory Primary inventory management folder High"},{"location":"transition_plan/#system-comparison-great-lakes-vs-armis2","title":"System Comparison: Great Lakes vs. Armis2","text":"Feature Great Lakes Armis2 Purpose General HPC Secure, HIPAA/sensitive data Mount Path /nfs/turbo/umms-TEMPNAME /nfs/turbo/umms-TEMPNAME Data Type Research, analysis Sensitive, regulated Status Work in progress Work in progress"},{"location":"transition_plan/#transition-priorities-matrix","title":"Transition Priorities &amp; Matrix","text":"Item Criticality Frequency of Use Priority Inventory.xlsx High High High samples_and_purification.xlsx High High High Turbo Storage High Medium High OneDrive Medium Medium Medium R Drive Low Low Low"},{"location":"transition_plan/#timeline-handover-checklist","title":"Timeline &amp; Handover Checklist","text":"<pre><code>gantt\n    title Transition Timeline\n    dateFormat  YYYY-MM-DD\n    section Documentation\n    Review Inventory Files      :done,    des1, 2025-08-22, 2d\n    Document OneDrive/R Drive   :active,  des2, 2025-08-24, 3d\n    Update Turbo/Cluster Info   :         des3, 2025-08-27, 2d\n    Final Review &amp; Handover     :         des4, 2025-08-29, 1d\n</code></pre> <ul> <li>[ ] Review and update all inventory files</li> <li>[ ] Complete documentation for OneDrive and R Drive</li> <li>[ ] Confirm Turbo storage and cluster access</li> <li>[ ] Finalize handover with successor</li> </ul>"},{"location":"transition_plan/#glossary","title":"Glossary","text":"<ul> <li>HPC: High-Performance Computing</li> <li>Turbo: U-M high-performance storage system</li> <li>Globus: Data transfer tool for research computing</li> <li>Dataden: U-M data infrastructure for sharing datasets</li> <li>Great Lakes: General-purpose U-M HPC cluster</li> <li>Armis2: Secure U-M HPC cluster for sensitive data</li> </ul>"},{"location":"transition_plan/#troubleshooting","title":"Troubleshooting","text":"<p> Common Issues:</p> <ul> <li>Trouble accessing Turbo storage? Check VPN and permissions.</li> <li>OneDrive sync issues? Restart client and check login.</li> <li>Globus transfer errors? Verify endpoint and credentials.</li> </ul>"},{"location":"transition_plan/#contact-information","title":"Contact Information","text":"<ul> <li>Lab Manager: [Name Placeholder] ([email@umich.edu])</li> <li>IT Support: [IT Contact Placeholder] ([it-support@umich.edu])</li> <li>ARC Help: https://arc.umich.edu/contact/</li> </ul>"},{"location":"transition_plan/#next-steps-action-items","title":"Next Steps / Action Items","text":"<ul> <li>Review and update all documentation regularly</li> <li>Ensure backups are current and accessible</li> <li>Schedule periodic access reviews for all storage systems</li> <li>Provide feedback and suggestions for future improvements</li> </ul> <p> Note: This document is a living resource. Update as needed to reflect changes in lab infrastructure or personnel.</p>"}]}